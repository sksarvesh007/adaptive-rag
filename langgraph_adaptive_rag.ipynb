{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R4rLJwN-jSwP"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4egPR7fgj14P"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_google_genai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smf-Ambwmlx5",
        "outputId": "ec2dbec0-3b59-47b3-b01f-a3c56370e1f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain_groq --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N_WK4W3-fBLm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "tavily_api_key = userdata.get(\"TAVILY_API_KEY\")\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "cohere_api_key = userdata.get(\"COHERE_API_KEY\")\n",
        "# Use the actual API key here:\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = userdata.get(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "langchain_api_key =userdata.get(\"LANGSMITH_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O3SUEsli5aj",
        "outputId": "7e8ab430-1c61-4886-be95-e4bf83f6010b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=userdata.get(\"GOOGLE_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_W3z1dIsgeZ_"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iqXv98j0kqA2"
      },
      "outputs": [],
      "source": [
        "google_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "embd = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    google_api_key=google_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T1-jNX1qkrjF"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsJspJvfk0p1",
        "outputId": "878c43be-bc36-4503-94e3-5e1d901e9d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "88\n"
          ]
        }
      ],
      "source": [
        "print(len(doc_splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ab5fBhLvk38J"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embd,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9P4zlwOk57g",
        "outputId": "a6bc2a9b-9242-4301-de7e-04377b436a9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model retrieved\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    groq_api_key = groq_api_key\n",
        ")\n",
        "print(\"model retrieved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHySTOyameKV",
        "outputId": "22a895be-f631-44da-f2d8-f5df966dc6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datasource='web_search'\n",
            "datasource='vectorstore'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Literal\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
        "    )\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    groq_api_key = groq_api_key\n",
        ")\n",
        "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
        "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
        "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_router = route_prompt | structured_llm_router\n",
        "print(\n",
        "    question_router.invoke(\n",
        "        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n",
        "    )\n",
        ")\n",
        "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw4RGW3znUXl",
        "outputId": "8e05f449-b849-40d0-f96c-03299043eb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    groq_api_key = groq_api_key\n",
        ")\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_e89BaZGZCik"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = langchain_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD-7w5PKWsSE",
        "outputId": "e9b5cab5-4c95-4455-fbb8-ac398d964018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the context of LLM-powered autonomous agents, memory refers to several types of information storage and retrieval processes. Sensory memory is the initial stage, holding impressions of sensory data for a few seconds. Short-term memory, also known as working memory, stores current awareness and cognitive task information for 20-30 seconds. Long-term memory, with unlimited capacity, stores facts, events, skills, and routines for extended periods. The agent's long-term memory is an external vector store, accessible via fast retrieval methods like Maximum Inner Product Search (MIPS).\n"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\" , api_key = langchain_api_key)\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unvvQ2OAYvPJ",
        "outputId": "6f6aad36-c24f-4890-b311-b68170ee6a6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    groq_api_key = groq_api_key\n",
        ")\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSSP8AUzggpC",
        "outputId": "2eff9d94-9c95-443a-e4ee-a57fcadfc813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75nzGK6MZsrC",
        "outputId": "e2e44455-c741-4df0-ffa4-4880d3c10468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradeAnswer(binary_score='yes')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm =  ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    groq_api_key = groq_api_key\n",
        ")\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lVkbt6zUdBSg",
        "outputId": "d4e1641a-1892-4e7f-fe1c-7b66fcb649a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"How can I access or inquire about the information stored in the memory of a language model agent?\"'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question. Just give me the question\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "k2PMlFI4evwA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool  = TavilySearchResults(k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a7X1QB2mYEf"
      },
      "source": [
        "# GRAPH\n",
        "capturing the flow of the graph\n",
        "\n",
        "## GRAPH STATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Eql1g8MlmKkN"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "  question : str\n",
        "  generation: str\n",
        "  documents : List[str]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDknjJeom0PZ"
      },
      "source": [
        "## GRAPH FLOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "juUZsSvSmuB7"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "def retrieve(state):\n",
        "  print(\"--RETRIEVING--\")\n",
        "  question = state[\"question\"]\n",
        "  documents = retriever.invoke(question)\n",
        "  return {\"documents\": documents ,\"question\":question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SVekUpHInYpj"
      },
      "outputs": [],
      "source": [
        "def generate(state):\n",
        "  print(\"--GENERATE--\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  generation = rag_chain.invoke({\"context\":documents , \"question\":question})\n",
        "  return {\"documents\": documents , \"question\":question , \"generation\":generation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Kehu-wGHog7j"
      },
      "outputs": [],
      "source": [
        "def grade_document(state):\n",
        "  print(\"----CHECKING DOCUMENT RELEVANCE----\")\n",
        "  documents = state[\"documents\"]\n",
        "  relevant_docs = []\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  for d in documents :\n",
        "    score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
        "    grade = score.binary_score\n",
        "    if grade == \"yes\":\n",
        "        print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "        relevant_docs.append(d)\n",
        "    else:\n",
        "        print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "        continue\n",
        "  return {\"documents\": relevant_docs, \"question\": question}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Sr42nUo5qPd2"
      },
      "outputs": [],
      "source": [
        "def transform_query(state):\n",
        "  print(\"--TRANSFORMING THE QUERY--\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "  better_question = question_rewriter.invoke({\"question\":question})\n",
        "  return {\"documents\": documents, \"question\": better_question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0by2eMCYqzbl"
      },
      "outputs": [],
      "source": [
        "def web_search(state):\n",
        "  print(\"-- DOING WEB SEARCH --\")\n",
        "  question = state[\"question\"]\n",
        "  docs = web_search_tool.invoke({\"query\": question})\n",
        "  web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "  web_results = Document(page_content = web_results)\n",
        "  return {\"documents\": web_results, \"question\": question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kgbFRVitr3kg"
      },
      "outputs": [],
      "source": [
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to web search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    if source.datasource == \"web_search\":\n",
        "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
        "        return \"web_search\"\n",
        "    elif source.datasource == \"vectorstore\":\n",
        "        print(\"---ROUTE QUESTION TO RAG---\")\n",
        "        return \"vectorstore\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mREecCDmt5fj"
      },
      "outputs": [],
      "source": [
        "def decide_to_generate(state):\n",
        "  \"\"\"\n",
        "  decides to whether to generate the answer or to re-generate the question\n",
        "  \"\"\"\n",
        "  print(\"--ASSESSING THE RELEVANT DOCUMENTS --\")\n",
        "  question = state[\"question\"]\n",
        "  relevant_docs = state[\"documents\"]\n",
        "  if not relevant_docs :\n",
        "    print(\"--DECISION : ALL THE DOCUMENT FOUND ARE NOT RELEVANT TO THE QUERY , SO WE NEED TO TRANSFORM IT \")\n",
        "    return \"transform_query\"\n",
        "  else :\n",
        "    print(\"--DECISION : GENERATE THE ANSWER\")\n",
        "    return \"generate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "gJfubfzDuwpo"
      },
      "outputs": [],
      "source": [
        "def grade_generation_v_documents_and_question(state):\n",
        "  print(\"--CHECKING THE HALLUCINATIONS--\")\n",
        "  question = state[\"question\"]\n",
        "  generation = state[\"generation\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  score = hallucination_grader.invoke({\"documents\": documents , \"generation\": generation})\n",
        "  grade = score.binary_score\n",
        "  if grade == \"yes\":\n",
        "    print(\"---GENERATION IS GROUNDED IN THE DOCUMENTS--- \")\n",
        "    print(\"--GRADING GENERATION VS QUESTION--\")\n",
        "    score = answer_grader.invoke({\"question\" :question , \"generation\" : generation})\n",
        "    grade = score.binary_score\n",
        "    if grade == \"yes\":\n",
        "      print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "      # Check question-answering\n",
        "      print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "      score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "      grade = score.binary_score\n",
        "      if grade == \"yes\":\n",
        "          print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "          return \"useful\"\n",
        "      else:\n",
        "          print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "          return \"not useful\"\n",
        "    else:\n",
        "      print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "      return \"not supported\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87-YNNwhw_ya"
      },
      "source": [
        "## BUILDING THE GRAPH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CHjZmE1xw9wh"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END , StateGraph , START\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "#defining the nodes\n",
        "\n",
        "#defining the nodes\n",
        "workflow.add_node(\"web_search\" , web_search)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"grade_documents\" , grade_document)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.add_node(\"transform_query\", transform_query)\n",
        "\n",
        "#building the graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"web_search\", \"generate\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCVHcmQ8yoxQ",
        "outputId": "b423d438-49f1-4e1c-f45e-4817e7f17946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO WEB SEARCH---\n",
            "\"Node 'question':\"\n",
            "'No generation found at this step.'\n",
            "{'question': 'who won the 2024 IPL ?'}\n",
            "'\\n---\\n'\n",
            "-- DOING WEB SEARCH --\n",
            "\"Node 'question':\"\n",
            "'No generation found at this step.'\n",
            "\"Node 'documents':\"\n",
            "'No generation found at this step.'\n",
            "{'question': 'who won the 2024 IPL ?', 'documents': Document(page_content=\"KKR vs SRH IPL 2024 Final Highlights: Kolkata Knight Riders beat SunRisers Hyderabad by 8 wickets in the final of IPL 2024 to clinch their third title on Sunday. Chasing a paltry target of 114 ...\\nIPL 2024 Final, KKR vs SRH: The Knight Riders won their third title after hammering SunRisers in the most one-sided final in the history of the tournament. Venkatesh Iyer's fifty and an all-round bowling show helped KKR hammer SRH in the final in Chennai on Sunday, May 26.\\nIPL Final 2024, KKR vs SRH Live Score: The biggest win in an IPL final ever IPL Final 2024, KKR vs SRH Live Score: KKR have won this mathc with 57 balls to spare which is the most in any match in ...\\nIPL Final 2024 result, score. SRH: 113/10 in 18.3 overs (Cummins 24, Markram 20, Russell 3/19) KKR: 114/2 in 10.3 overs (Iyer 51, Gurbaz 39, Cummins 1/18) READ MORE: Who are the top run scorers in ...\\nKKR vs SRH, IPL 2024 Final Highlights: Venkatesh Iyer's fifty helps Kolkata Knight Riders beat Sunrisers Hyderabad for third IPL title KKR vs SRH, IPL 2024: Get the IPL score updates and highlights from the final between Kolkata Knight Riders and Sunrisers Hyderabad happening at the MA Chidambaram Stadium.\")}\n",
            "'\\n---\\n'\n",
            "--GENERATE--\n",
            "--CHECKING THE HALLUCINATIONS--\n",
            "---GENERATION IS GROUNDED IN THE DOCUMENTS--- \n",
            "--GRADING GENERATION VS QUESTION--\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'question':\"\n",
            "'No generation found at this step.'\n",
            "\"Node 'generation':\"\n",
            "'No generation found at this step.'\n",
            "\"Node 'documents':\"\n",
            "'No generation found at this step.'\n",
            "{'question': 'who won the 2024 IPL ?', 'generation': 'The Kolkata Knight Riders won the 2024 Indian Premier League (IPL) by beating the SunRisers Hyderabad by 8 wickets. This victory marked their third title in the tournament. The match took place at the MA Chidambaram Stadium.', 'documents': Document(page_content=\"KKR vs SRH IPL 2024 Final Highlights: Kolkata Knight Riders beat SunRisers Hyderabad by 8 wickets in the final of IPL 2024 to clinch their third title on Sunday. Chasing a paltry target of 114 ...\\nIPL 2024 Final, KKR vs SRH: The Knight Riders won their third title after hammering SunRisers in the most one-sided final in the history of the tournament. Venkatesh Iyer's fifty and an all-round bowling show helped KKR hammer SRH in the final in Chennai on Sunday, May 26.\\nIPL Final 2024, KKR vs SRH Live Score: The biggest win in an IPL final ever IPL Final 2024, KKR vs SRH Live Score: KKR have won this mathc with 57 balls to spare which is the most in any match in ...\\nIPL Final 2024 result, score. SRH: 113/10 in 18.3 overs (Cummins 24, Markram 20, Russell 3/19) KKR: 114/2 in 10.3 overs (Iyer 51, Gurbaz 39, Cummins 1/18) READ MORE: Who are the top run scorers in ...\\nKKR vs SRH, IPL 2024 Final Highlights: Venkatesh Iyer's fifty helps Kolkata Knight Riders beat Sunrisers Hyderabad for third IPL title KKR vs SRH, IPL 2024: Get the IPL score updates and highlights from the final between Kolkata Knight Riders and Sunrisers Hyderabad happening at the MA Chidambaram Stadium.\")}\n",
            "'\\n---\\n'\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "inputs = {\n",
        "    \"question\": \"who won the 2024 IPL ?\"\n",
        "}\n",
        "\n",
        "for output in app.stream(inputs, stream_mode=\"values\"):\n",
        "    for key, value in output.items():\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Ensure 'generation' key exists\n",
        "        if \"generation\" in value:\n",
        "            pprint(value[\"generation\"])\n",
        "        else:\n",
        "            pprint(\"No generation found at this step.\")\n",
        "    print(output)\n",
        "    pprint(\"\\n---\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cor1utPrZeNi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
